#include <math.h>
#include "mpi.h"
#include <stdio.h>
#include <stdlib.h>

#define ROOT 0
#define SECOND_CLUSTER 1
#define THIRD_CLUSTER 2
#define N_CLUSTERS 3
#define FNAME_SIZE 13
#define MIN(x, y) ((x < y) ? x : y)

/**
 * @brief Each worker receives from its coordinator the array computet by ROOT
 * process (rank 0) and its size
 *
 * @param N 				size of the array
 * @param v					array generated by the ROOT process
 * @param workers 			workers of all clusters
 * @param workers_count		cluster number of workers
 * @param src 				cluster coordinator rank
 */
void inform_workers(int N, int* v, int* workers[N_CLUSTERS], int workers_count,
	int src) {
	for (int i = 0; i < workers_count; i++) {
		MPI_Send(&N, 1, MPI_INT, workers[src][i], 0, MPI_COMM_WORLD);
		printf("M(%d,%d)\n", src, workers[src][i]);
		MPI_Send(v, N, MPI_INT, workers[src][i], 0, MPI_COMM_WORLD);
		printf("M(%d,%d)\n", src, workers[src][i]);
	}
}


/**
 * @brief Set the sizes for the cluster coordinator processes in order to print
 * the topology correctly
 *
 * @param src 						cluster coordinator process
 * @param root_workers 				number of workers of the root cluster (0)
 * @param second_cluster_workers 	number of workers of the second cluster (1)
 * @param third_cluster_workers 	number of workers of the third cluster (2)
 * @param s1 						number of workers of the cluster coordinator process
 * @param s2 						number of workers of another cluster
 * @param s3 						number of workers of the last cluster
 */
void set_sizes(int src, int* root_workers, int* second_cluster_workers, int* third_cluster_workers, int w1, int w2, int w3) {
	if (src == ROOT) {
		*root_workers = w1;
		*second_cluster_workers = w2;
		*third_cluster_workers = w3;
	}

	if (src == SECOND_CLUSTER) {
		*second_cluster_workers = w1;
		*root_workers = w2;
		*third_cluster_workers = w3;
	}

	if (src == THIRD_CLUSTER) {
		*third_cluster_workers = w1;
		*root_workers = w2;
		*second_cluster_workers = w3;
	}
}


/**
 * @brief Iterate through the clusters workers and print the topology in the desired format
 *
 * @param rank 						process rank
 * @param workers 					workers of all clusters
 * @param root_workers 				number of workers of the root cluster (0)
 * @param second_cluster_workers 	number of workers of the second cluster (1)
 * @param third_cluster_workers 	number of workers of the third cluster (2)
 */
void print_topology(int rank, int* workers[N_CLUSTERS], int root_workers, int second_cluster_workers, int third_cluster_workers) {
	printf("%d -> ", rank);
	for (int i = 0; i < N_CLUSTERS; i++) {
		printf("%d:", i);
		int size;
		if (i == ROOT) {
			size = root_workers;
		}
		else if (i == SECOND_CLUSTER) {
			size = second_cluster_workers;
		}
		else {
			size = third_cluster_workers;
		}
		for (int j = 0; j < size; j++) {
			if (j != size - 1) {
				printf("%d,", workers[i][j]);
			}
			else {
				printf("%d ", workers[i][j]);
			}
		}
	}
	printf("\n");
}


/**
 * @brief Establishes the topology of the distributed system for all processes
 *
 * @param src 						source cluster
 * @param c1 						first target cluster
 * @param c2 						second target cluster
 * @param workers 					workers of all clusters
 * @param workers_count 			cluster number of workers
 */
void find_topology(int src, int c1, int c2, int* workers[N_CLUSTERS],
	int workers_count) {
	int i, root_cluster_size, second_cluster_size, third_cluster_size;
	// Send dimension of the current cluster to the other two coordinator
	// processes
	MPI_Send(&workers_count, 1, MPI_INT, c1, 0, MPI_COMM_WORLD);
	printf("M(%d,%d)\n", src, c1);
	MPI_Send(&workers_count, 1, MPI_INT, c2, 0, MPI_COMM_WORLD);
	printf("M(%d,%d)\n", src, c2);

	// Send the current cluster elements to these two processes
	for (i = 0; i < workers_count; i++) {
		MPI_Send(&workers[src][i], 1, MPI_INT, c1, 0, MPI_COMM_WORLD);
		printf("M(%d,%d)\n", src, c1);
		MPI_Send(&workers[src][i], 1, MPI_INT, c2, 0, MPI_COMM_WORLD);
		printf("M(%d,%d)\n", src, c2);
	}

	MPI_Status status;
	int c1_size, c2_size;
	// Receive each size for the other two clusters
	MPI_Recv(&c1_size, 1, MPI_INT, c1, 0, MPI_COMM_WORLD, &status);
	MPI_Recv(&c2_size, 1, MPI_INT, c2, 0, MPI_COMM_WORLD, &status);

	workers[c1] = malloc(c1_size * sizeof(int));
	workers[c2] = malloc(c2_size * sizeof(int));

	// Receive elements of the clusters
	for (i = 0; i < c1_size; i++) {
		MPI_Recv(&workers[c1][i], 1, MPI_INT, c1, 0, MPI_COMM_WORLD, &status);
	}

	for (i = 0; i < c2_size; i++) {
		MPI_Recv(&workers[c2][i], 1, MPI_INT, c2, 0, MPI_COMM_WORLD, &status);
	}

	set_sizes(src, &root_cluster_size, &second_cluster_size, &third_cluster_size, workers_count, c1_size, c2_size);
	
	// Print the topology in the desired format
	print_topology(src, workers, root_cluster_size, second_cluster_size, third_cluster_size);

	// Send topology to workers
	for (i = 0; i < workers_count; i++) {
		MPI_Send(&root_cluster_size, 1, MPI_INT, workers[src][i], 0, MPI_COMM_WORLD);
		printf("M(%d,%d)\n", src, workers[src][i]);

		MPI_Send(&second_cluster_size, 1, MPI_INT, workers[src][i], 0, MPI_COMM_WORLD);
		printf("M(%d,%d)\n", src, workers[src][i]);

		MPI_Send(&third_cluster_size, 1, MPI_INT, workers[src][i], 0, MPI_COMM_WORLD);
		printf("M(%d,%d)\n", src, workers[src][i]);

		MPI_Send(workers[ROOT], root_cluster_size, MPI_INT, workers[src][i], 0, MPI_COMM_WORLD);
		printf("M(%d,%d)\n", src, workers[src][i]);

		MPI_Send(workers[SECOND_CLUSTER], second_cluster_size, MPI_INT, workers[src][i], 0, MPI_COMM_WORLD);
		printf("M(%d,%d)\n", src, workers[src][i]);

		MPI_Send(workers[THIRD_CLUSTER], third_cluster_size, MPI_INT, workers[src][i], 0, MPI_COMM_WORLD);
		printf("M(%d,%d)\n", src, workers[src][i]);
	}
}


/**
 * @brief Gather partial results from workers and place them in coordinator's local/partial solution
 *
 * @param N 						size of the array computed by ROOT process
 * @param workers_count 			cluster number of workers
 * @param sizes 					array that contains the number of iterations of every worker
 * @param sol 						cluster partial solution
 * @param process_rank 				rank of coordinator process
 */
void get_workers_result(int N, int workers_count, int* sizes, int* sol[], int process_rank) {
	for (int i = 0; i < workers_count; i++) {
		MPI_Status status;
		int worker_rank, size;

		MPI_Recv(&worker_rank, 1, MPI_INT, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &status);
		MPI_Recv(&size, 1, MPI_INT, worker_rank, 0, MPI_COMM_WORLD, &status);

		sizes[worker_rank - N_CLUSTERS] = size;

		if (size > 0) {
			sol[worker_rank - N_CLUSTERS] = malloc(size * sizeof(int));
			for (int j = 0; j < size; j++) {
				MPI_Recv(&sol[worker_rank - N_CLUSTERS][j], 1, MPI_INT, worker_rank, 0, MPI_COMM_WORLD, &status);
			}
		}
	}
}


/**
 * @brief Root process receives partial solution from cluster coordinator
 * 		  and computes the final solution
 *
 * @param workers_count 			cluster number of workers
 * @param source_cluster 			cluster coordinator
 * @param sizes 					array that contains the number of iterations of every worker
 * @param sol 						cluster partial solution - in this case, final solution
 */
void receive_partial_sol_from_cluster(int workers_count, int source_cluster, int* sizes, int* sol[]) {
	MPI_Status status;
	for (int i = 0; i < workers_count; i++) {
		int worker_id = 0;

		MPI_Recv(&worker_id, 1, MPI_INT, source_cluster, 0, MPI_COMM_WORLD, &status);
		MPI_Recv(&sizes[worker_id], 1, MPI_INT, source_cluster, 0, MPI_COMM_WORLD, &status);

		sol[worker_id] = malloc(sizes[worker_id] * sizeof(int));
		for (int j = 0; j < sizes[worker_id]; j++) {
			MPI_Recv(&sol[worker_id][j], 1, MPI_INT, source_cluster, 0, MPI_COMM_WORLD, &status);
		}
	}
}


/**
 * @brief Send cluster local solution to another cluster coordinator process
 *
 * @param N 						size of the array computed by ROOT process
 * @param workers_count 			cluster number of workers
 * @param process_rank 				rank of coordinator process
 * @param sizes 					array that contains the number of iterations of every worker
 * @param sol 						cluster partial solution
 * @param target 					cluster coordinator that receives the partial solution
 */
void send_cluster_solution(int N, int workers_count, int process_rank, int* sizes, int* sol[], int target) {

	// Send the number of workers so that the target cluster knows how many receives
	// need to take place
	MPI_Send(&workers_count, 1, MPI_INT, target, 0, MPI_COMM_WORLD);
	printf("M(%d,%d)\n", process_rank, target);

	// Send worker's id (array index), number of iterations/elements computed 
	// by it and the elements' values
	for (int i = 0; i < N; i++) {
		if (sizes[i] > 0) {
			int id = i;
			MPI_Send(&id, 1, MPI_INT, target, 0, MPI_COMM_WORLD);
			printf("M(%d,%d)\n", process_rank, target);

			MPI_Send(&sizes[i], 1, MPI_INT, target, 0, MPI_COMM_WORLD);
			printf("M(%d,%d)\n", process_rank, target);

			for (int j = 0; j < sizes[i]; j++) {
				MPI_Send(&sol[i][j], 1, MPI_INT, target, 0, MPI_COMM_WORLD);
				printf("M(%d,%d)\n", process_rank, target);
			}
		}
	}
}


int main(int argc, char* argv[]) {
	int nProcesses, rank;
	MPI_Init(&argc, &argv);
	MPI_Comm_size(MPI_COMM_WORLD, &nProcesses);
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);

	int i, j;
	FILE* infile;
	char infile_name[FNAME_SIZE];
	int* v, N;
	int* workers[N_CLUSTERS];

	if (rank == ROOT || rank == SECOND_CLUSTER || rank == THIRD_CLUSTER) {

		int workers_count, process_rank = rank;
		// Create the file name for each cluster
		sprintf(infile_name, "cluster%i.txt", process_rank);

		infile = fopen(infile_name, "r");
		fscanf(infile, "%d", &workers_count);

		workers[process_rank] = malloc(workers_count * sizeof(int));

		for (i = 0; i < workers_count; i++) {
			fscanf(infile, "%d", &workers[process_rank][i]);
			// Notify each worker which process is their coordinator
			MPI_Send(&process_rank, 1, MPI_INT, workers[process_rank][i], 0, MPI_COMM_WORLD);
			printf("M(%d,%d)\n", process_rank, workers[process_rank][i]);
		}

		if (process_rank == ROOT) {
			find_topology(ROOT, SECOND_CLUSTER, THIRD_CLUSTER, workers, workers_count);
		}
		else if (process_rank == SECOND_CLUSTER) {
			find_topology(SECOND_CLUSTER, ROOT, THIRD_CLUSTER, workers, workers_count);
		}
		else {
			find_topology(THIRD_CLUSTER, ROOT, SECOND_CLUSTER, workers, workers_count);
		}

		fclose(infile);

		if (process_rank == ROOT) {
			N = atoi(argv[1]);
			v = (int*)malloc(N * sizeof(int));

			int* sol[N];
			int sizes[N];

			for (i = 0; i < N; i++) {
				v[i] = i;
				sizes[i] = 0;
			}

			// Send array to the other two coordinators
			for (i = 1; i < N_CLUSTERS; i++) {
				MPI_Send(&N, 1, MPI_INT, i, 0, MPI_COMM_WORLD);
				printf("M(%d,%d)\n", process_rank, i);
				MPI_Send(v, N, MPI_INT, i, 0, MPI_COMM_WORLD);
				printf("M(%d,%d)\n", process_rank, i);
			}

			// Send the array and its size to the workers
			inform_workers(N, v, workers, workers_count, ROOT);

			// Root process receives the count of worker processes for the other two clusters
			int workers_second_cluster, workers_third_cluster;
			MPI_Status status;
			MPI_Recv(&workers_second_cluster, 1, MPI_INT, SECOND_CLUSTER, 0, MPI_COMM_WORLD, &status);
			MPI_Recv(&workers_third_cluster, 1, MPI_INT, THIRD_CLUSTER, 0, MPI_COMM_WORLD, &status);

			// Receive partial solution from first cluster
			receive_partial_sol_from_cluster(workers_second_cluster, SECOND_CLUSTER, sizes, sol);

			// Receive partial solution from second cluster
			receive_partial_sol_from_cluster(workers_third_cluster, THIRD_CLUSTER, sizes, sol);

			// Coordinator receives partial results from their workers and stores
			// them in their local partial solution
			get_workers_result(N, workers_count, sizes, sol, process_rank);

			// Print result
			printf("Rezultat: ");
			for (i = 0; i < N; i++) {
				for (j = 0; j < sizes[i]; j++) {
					printf("%d ", sol[i][j]);
				}
			}
			printf("\n");
		}
		else {
			MPI_Status status;
			MPI_Recv(&N, 1, MPI_INT, ROOT, 0, MPI_COMM_WORLD, &status);

			v = (int*)malloc(N * sizeof(int));
			MPI_Recv(v, N, MPI_INT, ROOT, 0, MPI_COMM_WORLD, &status);

			// Send the array and its size to the workers
			inform_workers(N, v, workers, workers_count, process_rank);

			int* sol[N];
			int sizes[N];

			for (i = 0; i < N; i++) {
				sizes[i] = 0;
			}

			// Coordinator receives partial results from their workers and stores
			// them in their local partial solution
			get_workers_result(N, workers_count, sizes, sol, process_rank);

			// After the coordinator collects all the partial results from the workers
			// it sends the local solution to the main process, ROOT
			send_cluster_solution(N, workers_count, process_rank, sizes, sol, ROOT);
		}
	}
	else {
		// Worker computes the values
		int coordinator_rank, src = MPI_ANY_SOURCE;
		MPI_Status status;
		MPI_Recv(&coordinator_rank, 1, MPI_INT, src, 0, MPI_COMM_WORLD, &status);
		src = coordinator_rank;

		// Receive topology
		int root_workers, second_cluster_workers, third_cluster_workers;
		MPI_Recv(&root_workers, 1, MPI_INT, src, 0, MPI_COMM_WORLD, &status);
		MPI_Recv(&second_cluster_workers, 1, MPI_INT, src, 0, MPI_COMM_WORLD, &status);
		MPI_Recv(&third_cluster_workers, 1, MPI_INT, src, 0, MPI_COMM_WORLD, &status);

		workers[ROOT] = malloc(root_workers * sizeof(int));
		workers[SECOND_CLUSTER] = malloc(second_cluster_workers * sizeof(int));
		workers[THIRD_CLUSTER] = malloc(third_cluster_workers * sizeof(int));

		MPI_Recv(workers[ROOT], root_workers, MPI_INT, src, 0, MPI_COMM_WORLD, &status);
		MPI_Recv(workers[SECOND_CLUSTER], second_cluster_workers, MPI_INT, src, 0, MPI_COMM_WORLD, &status);
		MPI_Recv(workers[THIRD_CLUSTER], third_cluster_workers, MPI_INT, src, 0, MPI_COMM_WORLD, &status);

		print_topology(rank, workers, root_workers, second_cluster_workers, third_cluster_workers);

		// Receive the array generated by ROOT process and its size		
		MPI_Recv(&N, 1, MPI_INT, src, 0, MPI_COMM_WORLD, &status);

		v = (int*)malloc(N * sizeof(int));
		MPI_Recv(v, N, MPI_INT, src, 0, MPI_COMM_WORLD, &status);

		// Compute new values for array elements
		int id = rank - N_CLUSTERS;
		int start = id * ceil((double)N / (nProcesses - N_CLUSTERS));
		int end = MIN((id + 1) * ceil((double)N / (nProcesses - N_CLUSTERS)), N);
		int size = end - start;
		int* partial_res = (int*)malloc(size * sizeof(int));

		int count = 0;
		for (i = start; i < end; i++) {
			partial_res[count] = v[i] * 2;
			count++;
		}

		// Send to coordinator the worker's ID, the size of the partial result
		// and the elements computed
		MPI_Send(&rank, 1, MPI_INT, src, 0, MPI_COMM_WORLD);
		printf("M(%d,%d)\n", rank, src);

		MPI_Send(&size, 1, MPI_INT, src, 0, MPI_COMM_WORLD);
		printf("M(%d,%d)\n", rank, src);

		if (size > 0) {
			for (i = 0; i < size; i++) {
				MPI_Send(&partial_res[i], 1, MPI_INT, src, 0, MPI_COMM_WORLD);
				printf("M(%d,%d)\n", rank, src);
			}
		}
	}

	MPI_Finalize();
}
